{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "local-battle",
   "metadata": {},
   "source": [
    "# SQPLM\n",
    "\n",
    "可用的中文预训练参数：[`bert-base`](https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip),[`roberta-wwm-ext-base`](https://drive.google.com/uc?export=download&id=1jMAKIJmPn7kADgD3yQZhpsqM-IRM1qZt),[`roberta-wwm-ext-large`](https://drive.google.com/uc?export=download&id=1dtad0FFzG11CBsawu8hvwwzU2R0FDI94),[`macbert-base`](https://drive.google.com/uc?export=download&id=1aV69OhYzIwj_hn-kO1RiBa-m8QAusQ5b),[`macbert-large`](https://drive.google.com/uc?export=download&id=1lWYxnk1EqTA2Q20_IShxBrCPc5VSDCkT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "rotary-academy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v2.5.20\n"
     ]
    }
   ],
   "source": [
    "import uf\n",
    "\n",
    "print(uf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "velvet-symbol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:LM Mode: `bi`. Use method `.to_mode()` to convert it into `bi`, `l2r`, `r2l` or `s2s`.\n",
      "uf.SQPLM(\n",
      "    config_file=\"../../ref/bert_config.json\",\n",
      "    vocab_file=\"../../ref/vocab.txt\",\n",
      "    max_seq_length=128,\n",
      "    label_size=None,\n",
      "    init_checkpoint=None,\n",
      "    output_dir=None,\n",
      "    gpu_ids=None,\n",
      "    drop_pooler=False,\n",
      "    do_sample_next_sentence=False,\n",
      "    max_predictions_per_seq=20,\n",
      "    masked_lm_prob=0.15,\n",
      "    short_seq_prob=0.1,\n",
      "    do_whole_word_mask=False,\n",
      "    mode=\"bi\",\n",
      "    do_lower_case=True,\n",
      "    truncate_method=\"LIFO\",\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = uf.SQPLM(\"../../ref/bert_config.json\", \"../../ref/vocab.txt\", do_sample_next_sentence=False)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-plumbing",
   "metadata": {},
   "source": [
    "可通过简单的一项指令，在四种训练模式中切换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "solved-behalf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to_mode(\"bi\")    # bi/l2r/r2l/s2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "drawn-breathing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bi/l2r/r2l模式下\n",
    "X = [\n",
    "     [\"天亮以前说再见，\", \"笑着泪流满面，\", \"去迎接应该你的，\", \"更好的明天。\"],     # 每一条样本是一个doc，doc内可以由多个句子组成\n",
    "     \"他想知道那是谁, 为何总沉默寡言, 人群中也算抢眼, 抢眼的孤独难免\",               # 也可以是一个完整的文段\n",
    "]\n",
    "\n",
    "# s2s模式下\n",
    "# X = [[\"天亮以前说再见\", \"笑着泪流满面\"], [\"他想知道那是谁\", \"为何总沉默寡言\"]]     # doc必须为两个句子"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-fault",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "excellent-religion",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/geyingli/Library/Python/3.8/lib/python/site-packages/uf-2.5.20-py3.8.egg/uf/apps/bert/bert.py:236: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  query_layer = tf.layers.dense(\n",
      "/Users/geyingli/Library/Python/3.8/lib/python/site-packages/uf-2.5.20-py3.8.egg/uf/apps/bert/bert.py:246: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  key_layer = tf.layers.dense(\n",
      "/Users/geyingli/Library/Python/3.8/lib/python/site-packages/uf-2.5.20-py3.8.egg/uf/apps/bert/bert.py:256: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  value_layer = tf.layers.dense(\n",
      "/Users/geyingli/Library/Python/3.8/lib/python/site-packages/uf-2.5.20-py3.8.egg/uf/apps/bert/bert.py:380: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  attention_output = tf.layers.dense(\n",
      "/Users/geyingli/Library/Python/3.8/lib/python/site-packages/uf-2.5.20-py3.8.egg/uf/apps/bert/bert.py:392: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  intermediate_output = tf.layers.dense(\n",
      "/Users/geyingli/Library/Python/3.8/lib/python/site-packages/uf-2.5.20-py3.8.egg/uf/apps/bert/bert.py:402: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  layer_output = tf.layers.dense(\n",
      "/Users/geyingli/Library/Python/3.8/lib/python/site-packages/uf-2.5.20-py3.8.egg/uf/apps/bert/bert.py:103: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  self.pooled_output = tf.layers.dense(\n",
      "/Users/geyingli/Library/Python/3.8/lib/python/site-packages/uf-2.5.20-py3.8.egg/uf/apps/bert/bert.py:468: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  input_tensor = tf.layers.dense(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Build graph with 308,647,326 parameters (among which 102,882,442 are trainable)\n",
      "INFO:tensorflow:Running local_init_op\n",
      "INFO:tensorflow:Done running local_init_op\n",
      "INFO:tensorflow:Running training on 2 samples (step 0 -> 20)\n",
      "INFO:tensorflow:step 1, MLM accuracy 0.0000, MLM loss 2.566339, NSP accuracy 0.0000, NSP loss 0.854683, 0.13 steps/sec, 0.26 examples/sec\n",
      "INFO:tensorflow:step 2, MLM accuracy 0.1000, MLM loss 2.195817, NSP accuracy 1.0000, NSP loss 0.119030, 0.48 steps/sec, 0.95 examples/sec\n",
      "INFO:tensorflow:step 3, MLM accuracy 0.8000, MLM loss 1.855942, NSP accuracy 1.0000, NSP loss 0.038016, 0.52 steps/sec, 1.03 examples/sec\n",
      "INFO:tensorflow:step 4, MLM accuracy 0.9000, MLM loss 1.634639, NSP accuracy 1.0000, NSP loss 0.022335, 0.52 steps/sec, 1.04 examples/sec\n",
      "INFO:tensorflow:step 5, MLM accuracy 0.8000, MLM loss 1.454528, NSP accuracy 1.0000, NSP loss 0.018225, 0.53 steps/sec, 1.05 examples/sec\n",
      "INFO:tensorflow:step 6, MLM accuracy 1.0000, MLM loss 1.301332, NSP accuracy 1.0000, NSP loss 0.012902, 0.54 steps/sec, 1.08 examples/sec\n",
      "INFO:tensorflow:step 7, MLM accuracy 1.0000, MLM loss 1.102295, NSP accuracy 1.0000, NSP loss 0.010501, 0.54 steps/sec, 1.07 examples/sec\n",
      "INFO:tensorflow:step 8, MLM accuracy 1.0000, MLM loss 0.933624, NSP accuracy 1.0000, NSP loss 0.008419, 0.53 steps/sec, 1.06 examples/sec\n",
      "INFO:tensorflow:step 9, MLM accuracy 1.0000, MLM loss 0.831077, NSP accuracy 1.0000, NSP loss 0.005426, 0.53 steps/sec, 1.06 examples/sec\n",
      "INFO:tensorflow:step 10, MLM accuracy 1.0000, MLM loss 0.735229, NSP accuracy 1.0000, NSP loss 0.004914, 0.54 steps/sec, 1.08 examples/sec\n",
      "INFO:tensorflow:step 11, MLM accuracy 1.0000, MLM loss 0.654896, NSP accuracy 1.0000, NSP loss 0.004293, 0.54 steps/sec, 1.08 examples/sec\n",
      "INFO:tensorflow:step 12, MLM accuracy 1.0000, MLM loss 0.586702, NSP accuracy 1.0000, NSP loss 0.003956, 0.54 steps/sec, 1.08 examples/sec\n",
      "INFO:tensorflow:step 13, MLM accuracy 1.0000, MLM loss 0.531923, NSP accuracy 1.0000, NSP loss 0.003250, 0.52 steps/sec, 1.04 examples/sec\n",
      "INFO:tensorflow:step 14, MLM accuracy 1.0000, MLM loss 0.485964, NSP accuracy 1.0000, NSP loss 0.003226, 0.54 steps/sec, 1.08 examples/sec\n",
      "INFO:tensorflow:step 15, MLM accuracy 1.0000, MLM loss 0.430757, NSP accuracy 1.0000, NSP loss 0.002856, 0.54 steps/sec, 1.07 examples/sec\n",
      "INFO:tensorflow:step 16, MLM accuracy 1.0000, MLM loss 0.410160, NSP accuracy 1.0000, NSP loss 0.002882, 0.54 steps/sec, 1.09 examples/sec\n",
      "INFO:tensorflow:step 17, MLM accuracy 1.0000, MLM loss 0.377175, NSP accuracy 1.0000, NSP loss 0.002441, 0.54 steps/sec, 1.08 examples/sec\n",
      "INFO:tensorflow:step 18, MLM accuracy 1.0000, MLM loss 0.362524, NSP accuracy 1.0000, NSP loss 0.002582, 0.53 steps/sec, 1.06 examples/sec\n",
      "INFO:tensorflow:step 19, MLM accuracy 1.0000, MLM loss 0.351902, NSP accuracy 1.0000, NSP loss 0.002617, 0.53 steps/sec, 1.07 examples/sec\n",
      "INFO:tensorflow:step 20, MLM accuracy 1.0000, MLM loss 0.328234, NSP accuracy 1.0000, NSP loss 0.002488, 0.54 steps/sec, 1.08 examples/sec\n"
     ]
    }
   ],
   "source": [
    "model.fit(X, [0] * len(X), total_steps=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-hundred",
   "metadata": {},
   "source": [
    "# 推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "opposite-pantyhose",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['天', '亮', '以', '前', '说', '再', '见', '，', '笑', '[MASK]', '泪', '流', '满', '面', '，', '[MASK]', '迎', '接', '应', '该', '你', '[MASK]', '[MASK]', '更', '好', '的', '明', '天', '。'], ['他', '想', '知', '[MASK]', '那', '是', '谁', ',', '[MASK]', '何', '总', '沉', '默', '寡', '言', ',', '人', '群', '[MASK]', '也', '算', '抢', '眼', ',', '[MASK]', '眼', '的', '[MASK]', '独', '难', '免']]\n",
      "INFO:tensorflow:Running inference on 2 samples\n",
      "INFO:tensorflow:process 100.0%, 1.56 examples/sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mlm_preds': [['的', '去', '的', '你'], ['道', '为', '中', '抢', '孤']],\n",
       " 'nsp_preds': [0, 0],\n",
       " 'nsp_probs': array([[0.99808383, 0.00191617],\n",
       "        [0.99836093, 0.00163909]], dtype=float32)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tokenized = [\n",
    "    [\n",
    "        token for token \n",
    "        in model.tokenizer.convert_ids_to_tokens(model.data[\"input_ids\"][i])\n",
    "        if token not in (\"[CLS]\", \"[SEP]\", \"[PAD]\")\n",
    "    ]\n",
    "    for i in range(len(model.data[\"input_ids\"]))\n",
    "]\n",
    "print(X_tokenized)\n",
    "model.predict(X_tokenized=X_tokenized)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d85a52a",
   "metadata": {},
   "source": [
    "# 训练样本生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c2b5344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens: ['[CLS]', '天', '亮', '以', '前', '说', '[MASK]', '见', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "\n",
      "input_tokens: ['[CLS]', '笑', '着', '[MASK]', '流', '满', '面', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "\n",
      "input_tokens: ['[CLS]', '去', '迎', '接', '应', '该', '你', '的', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "\n",
      "input_tokens: ['[CLS]', '[MASK]', '好', '的', '明', '天', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "\n",
      "input_tokens: ['[CLS]', '他', '[MASK]', '知', '道', '那', '是', '谁', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "\n",
      "input_tokens: ['[CLS]', '[MASK]', '何', '总', '沉', '默', '寡', '言', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "\n",
      "input_tokens: ['[CLS]', '人', '群', '中', '也', '算', '[MASK]', '眼', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "\n",
      "input_tokens: ['[CLS]', '抢', '眼', '的', '孤', '[MASK]', '难', '免', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = [\"天亮以前说再见\", \"笑着泪流满面\", \"去迎接应该你的\", \"更好的明天\", \"他想知道那是谁\", \"为何总沉默寡言\", \"人群中也算抢眼\", \"抢眼的孤独难免\"]\n",
    "data = model.convert(X, [0] * len(X), is_training=True)\n",
    "for i in range(len(data[\"input_ids\"])):\n",
    "    print(\"input_tokens: %s\" % (model.tokenizer.convert_ids_to_tokens(data[\"input_ids\"][i])))\n",
    "    print(\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
