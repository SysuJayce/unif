{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "urban-divorce",
   "metadata": {},
   "source": [
    "# RecBERT3LM\n",
    "\n",
    "可用的中文预训练参数：[`bert-base`](https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip),[`roberta-wwm-ext-base`](https://drive.google.com/uc?export=download&id=1jMAKIJmPn7kADgD3yQZhpsqM-IRM1qZt),[`roberta-wwm-ext-large`](https://drive.google.com/uc?export=download&id=1dtad0FFzG11CBsawu8hvwwzU2R0FDI94),[`macbert-base`](https://drive.google.com/uc?export=download&id=1aV69OhYzIwj_hn-kO1RiBa-m8QAusQ5b),[`macbert-large`](https://drive.google.com/uc?export=download&id=1lWYxnk1EqTA2Q20_IShxBrCPc5VSDCkT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "rotary-academy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v2.5.20\n"
     ]
    }
   ],
   "source": [
    "import uf\n",
    "\n",
    "print(uf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "velvet-symbol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uf.RecBERT3LM(\n",
      "    config_file=\"../../ref/bert_config.json\",\n",
      "    vocab_file=\"../../ref/vocab.txt\",\n",
      "    max_seq_length=10,\n",
      "    init_checkpoint=None,\n",
      "    output_dir=None,\n",
      "    gpu_ids=None,\n",
      "    add_prob=0.1,\n",
      "    rep_prob=0.1,\n",
      "    do_lower_case=True,\n",
      "    truncate_method=\"LIFO\",\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = uf.RecBERT3LM(\"../../ref/bert_config.json\", \"../../ref/vocab.txt\", max_seq_length=10)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "drawn-breathing",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [\"天亮以前说再见\", \"笑着泪流满面\", \"去迎接应该你的\", \"更好的明天\", \"他想知道那是谁\", \"为何总沉默寡言\", \"人群中也算抢眼\", \"抢眼的孤独难免\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-fault",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "excellent-religion",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/geyingli/Library/Python/3.8/lib/python/site-packages/uf-2.5.20-py3.8.egg/uf/apps/bert/bert.py:236: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  query_layer = tf.layers.dense(\n",
      "/Users/geyingli/Library/Python/3.8/lib/python/site-packages/uf-2.5.20-py3.8.egg/uf/apps/bert/bert.py:246: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  key_layer = tf.layers.dense(\n",
      "/Users/geyingli/Library/Python/3.8/lib/python/site-packages/uf-2.5.20-py3.8.egg/uf/apps/bert/bert.py:256: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  value_layer = tf.layers.dense(\n",
      "/Users/geyingli/Library/Python/3.8/lib/python/site-packages/uf-2.5.20-py3.8.egg/uf/apps/bert/bert.py:380: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  attention_output = tf.layers.dense(\n",
      "/Users/geyingli/Library/Python/3.8/lib/python/site-packages/uf-2.5.20-py3.8.egg/uf/apps/bert/bert.py:392: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  intermediate_output = tf.layers.dense(\n",
      "/Users/geyingli/Library/Python/3.8/lib/python/site-packages/uf-2.5.20-py3.8.egg/uf/apps/bert/bert.py:402: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  layer_output = tf.layers.dense(\n",
      "/Users/geyingli/Library/Python/3.8/lib/python/site-packages/uf-2.5.20-py3.8.egg/uf/apps/recbert/recbert3.py:173: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  logits = tf.layers.dense(\n",
      "/Users/geyingli/Library/Python/3.8/lib/python/site-packages/uf-2.5.20-py3.8.egg/uf/apps/recbert/recbert3.py:199: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  logits = tf.layers.dense(\n",
      "/Users/geyingli/Library/Python/3.8/lib/python/site-packages/uf-2.5.20-py3.8.egg/uf/apps/recbert/recbert3.py:206: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  logits = tf.layers.dense(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Build graph with 333,374,994 parameters (among which 111,124,998 are trainable)\n",
      "INFO:tensorflow:Running local_init_op\n",
      "INFO:tensorflow:Done running local_init_op\n",
      "INFO:tensorflow:Running training on 8 samples (step 0 -> 20)\n",
      "INFO:tensorflow:step 1, add_accuracy 0.3934, add_loss 0.780902, rep_accuracy 0.1475, rep_loss 0.962309, cls_accuracy 0.6250, cls_loss 0.616104, 0.13 steps/sec, 1.04 examples/sec\n",
      "INFO:tensorflow:step 2, add_accuracy 0.8361, add_loss 0.435689, rep_accuracy 0.8852, rep_loss 0.338141, cls_accuracy 0.8750, cls_loss 0.575802, 0.57 steps/sec, 4.55 examples/sec\n",
      "INFO:tensorflow:step 3, add_accuracy 0.8361, add_loss 0.398317, rep_accuracy 0.8852, rep_loss 0.336442, cls_accuracy 0.8750, cls_loss 0.409905, 0.64 steps/sec, 5.15 examples/sec\n",
      "INFO:tensorflow:step 4, add_accuracy 0.8361, add_loss 0.245317, rep_accuracy 0.8852, rep_loss 0.278587, cls_accuracy 0.8750, cls_loss 0.265004, 0.65 steps/sec, 5.23 examples/sec\n",
      "INFO:tensorflow:step 5, add_accuracy 0.8525, add_loss 0.215529, rep_accuracy 0.8852, rep_loss 0.143535, cls_accuracy 1.0000, cls_loss 0.194894, 0.64 steps/sec, 5.10 examples/sec\n",
      "INFO:tensorflow:step 6, add_accuracy 0.8525, add_loss 0.203694, rep_accuracy 0.8689, rep_loss 0.169181, cls_accuracy 1.0000, cls_loss 0.092929, 0.63 steps/sec, 5.04 examples/sec\n",
      "INFO:tensorflow:step 7, add_accuracy 0.9016, add_loss 0.160306, rep_accuracy 0.9344, rep_loss 0.201557, cls_accuracy 0.8750, cls_loss 0.132533, 0.64 steps/sec, 5.11 examples/sec\n",
      "INFO:tensorflow:step 8, add_accuracy 0.9344, add_loss 0.133114, rep_accuracy 0.9344, rep_loss 0.139485, cls_accuracy 1.0000, cls_loss 0.036533, 0.65 steps/sec, 5.17 examples/sec\n",
      "INFO:tensorflow:step 9, add_accuracy 0.9672, add_loss 0.110809, rep_accuracy 0.9508, rep_loss 0.087312, cls_accuracy 1.0000, cls_loss 0.037030, 0.63 steps/sec, 5.00 examples/sec\n",
      "INFO:tensorflow:step 10, add_accuracy 0.9508, add_loss 0.097941, rep_accuracy 0.9508, rep_loss 0.067255, cls_accuracy 1.0000, cls_loss 0.028782, 0.64 steps/sec, 5.16 examples/sec\n",
      "INFO:tensorflow:step 11, add_accuracy 0.9672, add_loss 0.096549, rep_accuracy 0.9344, rep_loss 0.071582, cls_accuracy 1.0000, cls_loss 0.025577, 0.66 steps/sec, 5.25 examples/sec\n",
      "INFO:tensorflow:step 12, add_accuracy 0.9672, add_loss 0.086607, rep_accuracy 0.9672, rep_loss 0.068018, cls_accuracy 1.0000, cls_loss 0.022898, 0.64 steps/sec, 5.11 examples/sec\n",
      "INFO:tensorflow:step 13, add_accuracy 0.9672, add_loss 0.086908, rep_accuracy 0.9836, rep_loss 0.056504, cls_accuracy 1.0000, cls_loss 0.008332, 0.63 steps/sec, 5.06 examples/sec\n",
      "INFO:tensorflow:step 14, add_accuracy 0.9508, add_loss 0.080524, rep_accuracy 0.9836, rep_loss 0.052292, cls_accuracy 1.0000, cls_loss 0.007740, 0.66 steps/sec, 5.28 examples/sec\n",
      "INFO:tensorflow:step 15, add_accuracy 0.9836, add_loss 0.075426, rep_accuracy 0.9836, rep_loss 0.056653, cls_accuracy 1.0000, cls_loss 0.005357, 0.64 steps/sec, 5.14 examples/sec\n",
      "INFO:tensorflow:step 16, add_accuracy 0.9672, add_loss 0.076293, rep_accuracy 0.9836, rep_loss 0.060857, cls_accuracy 1.0000, cls_loss 0.003262, 0.65 steps/sec, 5.24 examples/sec\n",
      "INFO:tensorflow:step 17, add_accuracy 0.9836, add_loss 0.065700, rep_accuracy 0.9836, rep_loss 0.062353, cls_accuracy 1.0000, cls_loss 0.002984, 0.66 steps/sec, 5.24 examples/sec\n",
      "INFO:tensorflow:step 18, add_accuracy 0.9836, add_loss 0.053256, rep_accuracy 0.9836, rep_loss 0.060802, cls_accuracy 1.0000, cls_loss 0.004609, 0.66 steps/sec, 5.26 examples/sec\n",
      "INFO:tensorflow:step 19, add_accuracy 0.9836, add_loss 0.058000, rep_accuracy 0.9836, rep_loss 0.053722, cls_accuracy 1.0000, cls_loss 0.002723, 0.66 steps/sec, 5.24 examples/sec\n",
      "INFO:tensorflow:step 20, add_accuracy 0.9672, add_loss 0.061892, rep_accuracy 0.9836, rep_loss 0.056150, cls_accuracy 1.0000, cls_loss 0.003317, 0.66 steps/sec, 5.26 examples/sec\n"
     ]
    }
   ],
   "source": [
    "model.reset()\n",
    "model.fit(X, total_steps=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-hundred",
   "metadata": {},
   "source": [
    "# 推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "opposite-pantyhose",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running inference on 8 samples\n",
      "INFO:tensorflow:process 100.0%, 6.72 examples/sec\n",
      "\n",
      "_input_tokens: ['', '更', '好', '的', '明', '天', '']\n",
      "_input_ids: [ 101 3291 1962 4638 3209 1921  102    0    0    0]\n",
      "_add_preds: [0 0 0 0 0 0 0 0 0 0]\n",
      "_rep_preds: [0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "_input_tokens: ['', '满', '中', '算', '眼', '']\n",
      "_input_ids: [ 101 4007  704 5050 4706  102    0    0    0    0]\n",
      "_add_preds: [   0 5408  738 2843    0    0 6241    0    0 6887]\n",
      "_rep_preds: [  0 782   0   0   0   0   0   0   0   0]\n",
      "\n",
      "_input_tokens: ['', '他', '应', '知', '那', '是', '谁', '免', '']\n",
      "_input_ids: [ 101  800 2418 4761 6929 3221 6443 1048  102    0]\n",
      "_add_preds: [   0    0    0 6887    0    0    0    0    0    0]\n",
      "_rep_preds: [   0    0 2682    0    0    0    0    1    0    1]\n",
      "\n",
      "_input_tokens: ['', '笑', '泪', '流', '满', '面', '']\n",
      "_input_ids: [ 101 5010 3801 3837 4007 7481  102    0    0    0]\n",
      "_add_preds: [   0 4708    0    0    0    0    0    0    0    0]\n",
      "_rep_preds: [0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "_input_tokens: ['', '抢', '算', '群', '孤', '独', '免', '']\n",
      "_input_ids: [ 101 2843 5050 5408 2109 4324 1048  102    0    0]\n",
      "_add_preds: [   0    0    0    0    0 7410    0    0    0    0]\n",
      "_rep_preds: [   0    0 4706 4638    0    0    0    0    0    0]\n",
      "\n",
      "_input_tokens: ['', '为', '何', '总', '何', '默', '寡', '']\n",
      "_input_ids: [ 101  711  862 2600  862 7949 2176  102    0    0]\n",
      "_add_preds: [   0    0    0    0    0    0 6241    0    0    0]\n",
      "_rep_preds: [0 0 0 0 1 0 0 0 0 0]\n",
      "\n",
      "_input_tokens: ['', '去', '迎', '应', '流', '该', '你', '的', '']\n",
      "_input_ids: [ 101 1343 6816 2418 3837 6421  872 4638  102    0]\n",
      "_add_preds: [   0    0 2970    0    0    0    0    0    0    0]\n",
      "_rep_preds: [0 0 0 0 1 0 0 0 0 0]\n",
      "\n",
      "_input_tokens: ['', '亮', '以', '前', '说', '再', '']\n",
      "_input_ids: [ 101  778  809 1184 6432 1086  102    0    0    0]\n",
      "_add_preds: [   0    0    0    0    0 6224    0    0    0 6224]\n",
      "_rep_preds: [0 0 0 0 0 0 0 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lm_preds': ['更好的明天',\n",
       "  '{rep:人}{add:群}中{add:也}算{add:抢}眼',\n",
       "  '他{rep:想}知{add:道}那是谁{rep:}',\n",
       "  '笑{add:着}泪流满面',\n",
       "  '抢{rep:眼}{rep:的}孤独{add:难}免',\n",
       "  '为何总{rep:}默寡{add:言}',\n",
       "  '去迎{add:接}应{rep:}该你的',\n",
       "  '亮以前说再{add:见}'],\n",
       " 'cls_probs': array([[9.9364495e-01, 6.3550863e-03],\n",
       "        [9.4027893e-04, 9.9905974e-01],\n",
       "        [6.9359323e-04, 9.9930644e-01],\n",
       "        [3.8290974e-03, 9.9617094e-01],\n",
       "        [7.4404030e-04, 9.9925596e-01],\n",
       "        [8.9427433e-04, 9.9910575e-01],\n",
       "        [2.1675352e-03, 9.9783248e-01],\n",
       "        [2.3811462e-03, 9.9761885e-01]], dtype=float32),\n",
       " 'cls_preds': [0, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [\n",
    "    \"\".join(\n",
    "        token for token \n",
    "        in model.tokenizer.convert_ids_to_tokens(model.data[\"input_ids\"][i])\n",
    "        if token not in (\"[CLS]\", \"[SEP]\", \"[PAD]\")\n",
    "    )\n",
    "    for i in range(len(model.data[\"input_ids\"]))\n",
    "]\n",
    "# X = [\n",
    "#     \"他想知道那是谁道\", \"为何总沉默寡\", \n",
    "#     \"群人面群中也抢面\", \"抢孤为独道难免\",\n",
    "# ]           # 人为增加一些错误，看模型是否能识别并纠正 (与训练阶段采样不同，因此此case预测错误正常)\n",
    "model.predict(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8c2ef60",
   "metadata": {},
   "source": [
    "# 训练样本生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b1f3617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens: ['[CLS]', '天', '言', '以', '前', '说', '再', '见', '[SEP]', '[PAD]']\n",
      "add_label_ids: ['[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "rep_label_ids: ['[PAD]', '[PAD]', '亮', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "\n",
      "input_tokens: ['[CLS]', '着', '泪', '满', '面', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "add_label_ids: ['笑', '[PAD]', '流', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "rep_label_ids: ['[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "\n",
      "input_tokens: ['[CLS]', '群', '去', '迎', '接', '该', '你', '的', '[SEP]', '[PAD]']\n",
      "add_label_ids: ['[PAD]', '[PAD]', '[PAD]', '[PAD]', '应', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "rep_label_ids: ['[PAD]', '[Soft Prompt]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "\n",
      "input_tokens: ['[CLS]', '更', '知', '的', '明', '天', '[SEP]', '[PAD]', '[PAD]', '[PAD]']\n",
      "add_label_ids: ['[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "rep_label_ids: ['[PAD]', '[PAD]', '好', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "\n",
      "input_tokens: ['[CLS]', '他', '想', '知', '道', '那', '是', '谁', '[SEP]', '[PAD]']\n",
      "add_label_ids: ['[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "rep_label_ids: ['[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "\n",
      "input_tokens: ['[CLS]', '为', '何', '总', '何', '默', '寡', '言', '[SEP]', '[PAD]']\n",
      "add_label_ids: ['[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "rep_label_ids: ['[PAD]', '[PAD]', '[PAD]', '[PAD]', '沉', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "\n",
      "input_tokens: ['[CLS]', '人', '群', '中', '也', '算', '抢', '[SEP]', '[PAD]', '[PAD]']\n",
      "add_label_ids: ['[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '眼', '[PAD]', '[PAD]', '[PAD]']\n",
      "rep_label_ids: ['[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "\n",
      "input_tokens: ['[CLS]', '抢', '眼', '的', '孤', '独', '难', '免', '[SEP]', '[PAD]']\n",
      "add_label_ids: ['[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "rep_label_ids: ['[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = [\"天亮以前说再见\", \"笑着泪流满面\", \"去迎接应该你的\", \"更好的明天\", \"他想知道那是谁\", \"为何总沉默寡言\", \"人群中也算抢眼\", \"抢眼的孤独难免\"]\n",
    "data = model.convert(X, is_training=True)\n",
    "for i in range(len(data[\"input_ids\"])):\n",
    "    print(\"input_tokens: %s\" % (model.tokenizer.convert_ids_to_tokens(data[\"input_ids\"][i])))\n",
    "    print(\"add_label_ids: %s\" % (model.tokenizer.convert_ids_to_tokens(data[\"add_label_ids\"][i])))\n",
    "    print(\"rep_label_ids: %s\" % (model.tokenizer.convert_ids_to_tokens(data[\"rep_label_ids\"][i])))\n",
    "    print(\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
