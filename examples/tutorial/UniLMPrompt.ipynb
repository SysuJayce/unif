{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "local-battle",
   "metadata": {},
   "source": [
    "# UniLMPrompt\n",
    "\n",
    "可用的中文预训练参数：[`bert-base`](https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip),[`roberta-wwm-ext-base`](https://drive.google.com/uc?export=download&id=1jMAKIJmPn7kADgD3yQZhpsqM-IRM1qZt),[`roberta-wwm-ext-large`](https://drive.google.com/uc?export=download&id=1dtad0FFzG11CBsawu8hvwwzU2R0FDI94),[`macbert-base`](https://drive.google.com/uc?export=download&id=1aV69OhYzIwj_hn-kO1RiBa-m8QAusQ5b),[`macbert-large`](https://drive.google.com/uc?export=download&id=1lWYxnk1EqTA2Q20_IShxBrCPc5VSDCkT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "rotary-academy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v2.5.13\n"
     ]
    }
   ],
   "source": [
    "import uf\n",
    "\n",
    "print(uf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb90e44",
   "metadata": {},
   "source": [
    "# 语言模型\n",
    "\n",
    "复用 `UniLM` 的 MASK 采样能力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28877f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:LM Mode: `bi`. Use method `.to_mode()` to convert it into `bi`, `l2r`, `r2l` or `s2s`.\n",
      "uf.UniLMPrompt(\n",
      "    config_file=\"../../ref/bert_config.json\",\n",
      "    vocab_file=\"../../ref/vocab.txt\",\n",
      "    max_seq_length=128,\n",
      "    init_checkpoint=None,\n",
      "    output_dir=None,\n",
      "    gpu_ids=None,\n",
      "    drop_pooler=False,\n",
      "    max_predictions_per_seq=20,\n",
      "    masked_lm_prob=0.15,\n",
      "    short_seq_prob=0.1,\n",
      "    do_whole_word_mask=False,\n",
      "    mode=\"bi\",\n",
      "    do_lower_case=True,\n",
      "    truncate_method=\"LIFO\",\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = uf.UniLMPrompt(\"../../ref/bert_config.json\", \"../../ref/vocab.txt\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "drawn-breathing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bi/l2r/r2l模式下\n",
    "model.to_mode(\"bi\")\n",
    "X_tokenized = [\n",
    "     [\n",
    "          [\"[Soft Prompt]\"],                                            # soft prompt\n",
    "          model.tokenizer.tokenize(\"被掩码的文字是？\"),                    # task description\n",
    "          model.tokenizer.tokenize(\"天亮以前说再见, 笑着泪流满面\"),         # 文本\n",
    "     ],\n",
    "     [\n",
    "          [\"[Soft Prompt]\"],                        \n",
    "          model.tokenizer.tokenize(\"被掩码的文字是？\"),                    \n",
    "          model.tokenizer.tokenize(\"他想知道那是谁, 为何总沉默寡言\"),\n",
    "     ],\n",
    "]\n",
    "\n",
    "# s2s模式下\n",
    "# model.to_mode(\"s2s\")\n",
    "# X_tokenized = [\n",
    "#      [\n",
    "#           [\"[Soft Prompt]\"],                                            # soft prompt\n",
    "#           model.tokenizer.tokenize(\"生成下一句歌词\"),                     # task description\n",
    "#           model.tokenizer.tokenize(\"天亮以前说再见\"),                     # 原始文本\n",
    "#           model.tokenizer.tokenize(\"笑着泪流满面\"),                       # 生成文本\n",
    "#      ],\n",
    "#      [\n",
    "#           [\"[Soft Prompt]\"],                        \n",
    "#           model.tokenizer.tokenize(\"生成下一句歌词\"),                     # task description\n",
    "#           model.tokenizer.tokenize(\"他想知道那是谁\"),                     # 原始文本\n",
    "#           model.tokenizer.tokenize(\"为何总沉默寡言\"),                     # 生成文本\n",
    "#      ],\n",
    "# ]\n",
    "\n",
    "y = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "808ef2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/geyingli/Library/Python/3.8/lib/python/site-packages/uf-2.5.13-py3.8.egg/uf/apps/bert/bert.py:235: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  query_layer = tf.layers.dense(\n",
      "/Users/geyingli/Library/Python/3.8/lib/python/site-packages/uf-2.5.13-py3.8.egg/uf/apps/bert/bert.py:245: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  key_layer = tf.layers.dense(\n",
      "/Users/geyingli/Library/Python/3.8/lib/python/site-packages/uf-2.5.13-py3.8.egg/uf/apps/bert/bert.py:255: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  value_layer = tf.layers.dense(\n",
      "/Users/geyingli/Library/Python/3.8/lib/python/site-packages/uf-2.5.13-py3.8.egg/uf/apps/bert/bert.py:379: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  attention_output = tf.layers.dense(\n",
      "/Users/geyingli/Library/Python/3.8/lib/python/site-packages/uf-2.5.13-py3.8.egg/uf/apps/bert/bert.py:391: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  intermediate_output = tf.layers.dense(\n",
      "/Users/geyingli/Library/Python/3.8/lib/python/site-packages/uf-2.5.13-py3.8.egg/uf/apps/bert/bert.py:401: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  layer_output = tf.layers.dense(\n",
      "/Users/geyingli/Library/Python/3.8/lib/python/site-packages/uf-2.5.13-py3.8.egg/uf/apps/bert/bert.py:102: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  self.pooled_output = tf.layers.dense(\n",
      "/Users/geyingli/Library/Python/3.8/lib/python/site-packages/uf-2.5.13-py3.8.egg/uf/apps/bert/bert.py:464: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  input_tensor = tf.layers.dense(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Build graph with 307,461,528 parameters (among which 102,880,904 are trainable)\n",
      "INFO:tensorflow:Running local_init_op\n",
      "INFO:tensorflow:Done running local_init_op\n",
      "INFO:tensorflow:Running training on 2 samples (step 0 -> 20)\n",
      "INFO:tensorflow:step 1, MLM accuracy 0.0000, MLM loss 1.018881, 0.14 steps/sec, 0.27 examples/sec\n",
      "INFO:tensorflow:step 2, MLM accuracy 0.7500, MLM loss 0.767338, 0.51 steps/sec, 1.02 examples/sec\n",
      "INFO:tensorflow:step 3, MLM accuracy 1.0000, MLM loss 0.587027, 0.49 steps/sec, 0.99 examples/sec\n",
      "INFO:tensorflow:step 4, MLM accuracy 1.0000, MLM loss 0.479584, 0.52 steps/sec, 1.04 examples/sec\n",
      "INFO:tensorflow:step 5, MLM accuracy 1.0000, MLM loss 0.403024, 0.52 steps/sec, 1.04 examples/sec\n",
      "INFO:tensorflow:step 6, MLM accuracy 1.0000, MLM loss 0.331306, 0.49 steps/sec, 0.98 examples/sec\n",
      "INFO:tensorflow:step 7, MLM accuracy 1.0000, MLM loss 0.270946, 0.52 steps/sec, 1.05 examples/sec\n",
      "INFO:tensorflow:step 8, MLM accuracy 1.0000, MLM loss 0.218453, 0.53 steps/sec, 1.05 examples/sec\n",
      "INFO:tensorflow:step 9, MLM accuracy 1.0000, MLM loss 0.183179, 0.47 steps/sec, 0.95 examples/sec\n",
      "INFO:tensorflow:step 10, MLM accuracy 1.0000, MLM loss 0.152208, 0.52 steps/sec, 1.03 examples/sec\n",
      "INFO:tensorflow:step 11, MLM accuracy 1.0000, MLM loss 0.123640, 0.49 steps/sec, 0.98 examples/sec\n",
      "INFO:tensorflow:step 12, MLM accuracy 1.0000, MLM loss 0.106241, 0.55 steps/sec, 1.09 examples/sec\n",
      "INFO:tensorflow:step 13, MLM accuracy 1.0000, MLM loss 0.090357, 0.55 steps/sec, 1.09 examples/sec\n",
      "INFO:tensorflow:step 14, MLM accuracy 1.0000, MLM loss 0.080118, 0.54 steps/sec, 1.08 examples/sec\n",
      "INFO:tensorflow:step 15, MLM accuracy 1.0000, MLM loss 0.068213, 0.53 steps/sec, 1.07 examples/sec\n",
      "INFO:tensorflow:step 16, MLM accuracy 1.0000, MLM loss 0.060832, 0.51 steps/sec, 1.01 examples/sec\n",
      "INFO:tensorflow:step 17, MLM accuracy 1.0000, MLM loss 0.054852, 0.51 steps/sec, 1.02 examples/sec\n",
      "INFO:tensorflow:step 18, MLM accuracy 1.0000, MLM loss 0.050350, 0.50 steps/sec, 1.00 examples/sec\n",
      "INFO:tensorflow:step 19, MLM accuracy 1.0000, MLM loss 0.046385, 0.52 steps/sec, 1.04 examples/sec\n",
      "INFO:tensorflow:step 20, MLM accuracy 1.0000, MLM loss 0.045626, 0.54 steps/sec, 1.08 examples/sec\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_tokenized=X_tokenized, total_steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1389349c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running inference on 1 samples\n",
      "INFO:tensorflow:process 100.0%, 0.88 examples/sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mlm_preds': [['前']]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tokenized = [\n",
    "     [\n",
    "          [\"[Soft Prompt]\"],                           \n",
    "          model.tokenizer.tokenize(\"被掩码的文字是？\"),\n",
    "          [\"他\", \"想\", \"[MASK]\", \"道\", \"那\", \"是\", \"谁\"],        # 手动赋予Mask (与训练阶段采样不同，因此此case预测错误正常)\n",
    "     ],\n",
    "]\n",
    "model.predict(X_tokenized=X_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-fault",
   "metadata": {},
   "source": [
    "# 下游任务\n",
    "\n",
    "大部分 NLP 任务都能够通过某种形式，转换为语言模型任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a55f070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:LM Mode: `bi`. Use method `.to_mode()` to convert it into `bi`, `l2r`, `r2l` or `s2s`.\n",
      "uf.UniLMPrompt(\n",
      "    config_file=\"../../ref/bert_config.json\",\n",
      "    vocab_file=\"../../ref/vocab.txt\",\n",
      "    max_seq_length=128,\n",
      "    init_checkpoint=None,\n",
      "    output_dir=None,\n",
      "    gpu_ids=None,\n",
      "    drop_pooler=False,\n",
      "    max_predictions_per_seq=20,\n",
      "    masked_lm_prob=0,\n",
      "    short_seq_prob=0.1,\n",
      "    do_whole_word_mask=False,\n",
      "    mode=\"bi\",\n",
      "    do_lower_case=True,\n",
      "    truncate_method=\"LIFO\",\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = uf.UniLMPrompt(\"../../ref/bert_config.json\", \"../../ref/vocab.txt\", masked_lm_prob=0)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a9db700",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tokenized = [\n",
    "     [\n",
    "          [\"[Soft Prompt]\"],                                                 # soft prompt\n",
    "          [\"情\", \"民\"],                                                       # verbalizer prompt\n",
    "          model.tokenizer.tokenize(\"下面这句歌词来自于\") + [\"[MASK]\", \"歌\"],    # task description\n",
    "          model.tokenizer.tokenize(\"天亮以前说再见, 笑着泪流满面\"),              # 文本\n",
    "     ],\n",
    "     [\n",
    "          [\"[Soft Prompt]\"],                        \n",
    "          [\"情\", \"民\"],                                                     \n",
    "          model.tokenizer.tokenize(\"下面这句歌词来自于\") + [\"[MASK]\", \"歌\"],          \n",
    "          model.tokenizer.tokenize(\"在天的尽头, 与月亮把盏\"),\n",
    "     ],\n",
    "]\n",
    "y = [[\"情\"], [\"民\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "660f97a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Build graph with 307,461,528 parameters (among which 102,880,904 are trainable)\n",
      "INFO:tensorflow:Running local_init_op\n",
      "INFO:tensorflow:Done running local_init_op\n",
      "INFO:tensorflow:Running training on 2 samples (step 0 -> 20)\n",
      "INFO:tensorflow:step 1, MLM accuracy 0.0000, MLM loss 0.531528, 0.13 steps/sec, 0.27 examples/sec\n",
      "INFO:tensorflow:step 2, MLM accuracy 1.0000, MLM loss 0.357832, 0.56 steps/sec, 1.11 examples/sec\n",
      "INFO:tensorflow:step 3, MLM accuracy 0.5000, MLM loss 0.275157, 0.56 steps/sec, 1.11 examples/sec\n",
      "INFO:tensorflow:step 4, MLM accuracy 0.5000, MLM loss 0.221405, 0.55 steps/sec, 1.09 examples/sec\n",
      "INFO:tensorflow:step 5, MLM accuracy 1.0000, MLM loss 0.189745, 0.55 steps/sec, 1.10 examples/sec\n",
      "INFO:tensorflow:step 6, MLM accuracy 0.5000, MLM loss 0.179693, 0.54 steps/sec, 1.09 examples/sec\n",
      "INFO:tensorflow:step 7, MLM accuracy 0.5000, MLM loss 0.150096, 0.55 steps/sec, 1.10 examples/sec\n",
      "INFO:tensorflow:step 8, MLM accuracy 0.5000, MLM loss 0.143473, 0.56 steps/sec, 1.11 examples/sec\n",
      "INFO:tensorflow:step 9, MLM accuracy 1.0000, MLM loss 0.132706, 0.54 steps/sec, 1.09 examples/sec\n",
      "INFO:tensorflow:step 10, MLM accuracy 0.5000, MLM loss 0.124280, 0.55 steps/sec, 1.10 examples/sec\n",
      "INFO:tensorflow:step 11, MLM accuracy 1.0000, MLM loss 0.116843, 0.55 steps/sec, 1.11 examples/sec\n",
      "INFO:tensorflow:step 12, MLM accuracy 0.5000, MLM loss 0.114731, 0.55 steps/sec, 1.10 examples/sec\n",
      "INFO:tensorflow:step 13, MLM accuracy 1.0000, MLM loss 0.103187, 0.55 steps/sec, 1.11 examples/sec\n",
      "INFO:tensorflow:step 14, MLM accuracy 1.0000, MLM loss 0.100737, 0.55 steps/sec, 1.11 examples/sec\n",
      "INFO:tensorflow:step 15, MLM accuracy 0.5000, MLM loss 0.101955, 0.55 steps/sec, 1.10 examples/sec\n",
      "INFO:tensorflow:step 16, MLM accuracy 0.5000, MLM loss 0.095180, 0.56 steps/sec, 1.11 examples/sec\n",
      "INFO:tensorflow:step 17, MLM accuracy 1.0000, MLM loss 0.092328, 0.52 steps/sec, 1.05 examples/sec\n",
      "INFO:tensorflow:step 18, MLM accuracy 0.5000, MLM loss 0.090994, 0.54 steps/sec, 1.09 examples/sec\n",
      "INFO:tensorflow:step 19, MLM accuracy 0.5000, MLM loss 0.093682, 0.55 steps/sec, 1.10 examples/sec\n",
      "INFO:tensorflow:step 20, MLM accuracy 1.0000, MLM loss 0.085143, 0.53 steps/sec, 1.05 examples/sec\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_tokenized=X_tokenized, y=y, total_steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0fc265b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running inference on 2 samples\n",
      "INFO:tensorflow:process 100.0%, 1.65 examples/sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mlm_preds': [['情'], ['民']]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_tokenized=X_tokenized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
